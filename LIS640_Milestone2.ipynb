{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a83ca55",
   "metadata": {},
   "source": [
    "## Milestone 2: Neural Network Baseline and Hyperparameter Optimization\n",
    "\n",
    "LIS 640 - Introduction to Applied Deep Learning\n",
    "\n",
    "Due 3/7/25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da321fe4",
   "metadata": {},
   "source": [
    "## **Overview**\n",
    "In Milestone 1 you have:\n",
    "1. **Defined a deep learning problem** where AI can make a meaningful impact.\n",
    "2. **Identified three datasets** that fit your topic and justified their relevance.\n",
    "3. **Explored and visualized** the datasets to understand their structure.\n",
    "4. **Implemented a PyTorch Dataset class** to prepare data for deep learning.\n",
    "\n",
    "In Milestone 2 we will take the next step and implement a neural network baseline based on what we have learned in class! For this milestone, please use one of the datasets you picked in the last milestone. If you pick a new one, make sure to do Steps 2 - 4 again. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba4078e",
   "metadata": {},
   "source": [
    "## **Step 1: Define Your Deep Learning Problem**\n",
    "\n",
    "The first step is to be clear about what you want your model to predict. Is your goal a classification or a regression task? what are the input features and what are you prediction targets y? Make sure that you have a sensible choice of features and a sensible choice of prediction targets y in your dataloader.\n",
    "\n",
    "**Write down one paragraph of justification for how you set up your DataLoader below. If it makes sense to change the DataLoader from Milestone 1, describe what you changed and why:**\n",
    "\n",
    "In this deep learning task, we are treating employee attrition prediction as a binary classification problem. Our DataLoader is set up to provide input features (x) that include key employee attributes such as age, gender, education level, performance rating, job satisfaction, compensation, and tenure, while the target variable (y) indicates whether an employee has left the company (1 for attrition, 0 for retention). Compared to the Milestone 1 DataLoader, I have made several modifications: numerical features are now normalized to mitigate scale differences, categorical features are one-hot encoded to better capture discrete information, and I plan to incorporate feature selection techniques in later stages to eliminate redundant or noisy variables. These changes ensure that the DataLoader delivers a more standardized and effective input, enhancing both the training efficiency and predictive performance of our deep learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bb8cc8",
   "metadata": {},
   "source": [
    "## **Step 2: Train a Neural Network in PyTorch**\n",
    "\n",
    "We learned in class how to implement and train a feed forward neural network in pytorch. You can find reference implementations [here](https://github.com/mariru/Intro2ADL/blob/main/Week5/Week5_Lab_Example.ipynb) and [here](https://www.kaggle.com/code/girlboss/mmlm2025-pytorch-lb-0-00000). Tip: Try to implement the neural network by yourself from scratch before looking at the reference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d971bc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (1470, 35)\n",
      "   Age Attrition     BusinessTravel  DailyRate              Department  \\\n",
      "0   41       Yes      Travel_Rarely       1102                   Sales   \n",
      "1   49        No  Travel_Frequently        279  Research & Development   \n",
      "2   37       Yes      Travel_Rarely       1373  Research & Development   \n",
      "3   33        No  Travel_Frequently       1392  Research & Development   \n",
      "4   27        No      Travel_Rarely        591  Research & Development   \n",
      "\n",
      "   DistanceFromHome  Education EducationField  EmployeeCount  EmployeeNumber  \\\n",
      "0                 1          2  Life Sciences              1               1   \n",
      "1                 8          1  Life Sciences              1               2   \n",
      "2                 2          2          Other              1               4   \n",
      "3                 3          4  Life Sciences              1               5   \n",
      "4                 2          1        Medical              1               7   \n",
      "\n",
      "   ...  RelationshipSatisfaction StandardHours  StockOptionLevel  \\\n",
      "0  ...                         1            80                 0   \n",
      "1  ...                         4            80                 1   \n",
      "2  ...                         2            80                 0   \n",
      "3  ...                         3            80                 0   \n",
      "4  ...                         4            80                 1   \n",
      "\n",
      "   TotalWorkingYears  TrainingTimesLastYear WorkLifeBalance  YearsAtCompany  \\\n",
      "0                  8                      0               1               6   \n",
      "1                 10                      3               3              10   \n",
      "2                  7                      3               3               0   \n",
      "3                  8                      3               3               8   \n",
      "4                  6                      3               3               2   \n",
      "\n",
      "  YearsInCurrentRole  YearsSinceLastPromotion  YearsWithCurrManager  \n",
      "0                  4                        0                     5  \n",
      "1                  7                        1                     7  \n",
      "2                  0                        0                     0  \n",
      "3                  7                        3                     0  \n",
      "4                  2                        2                     2  \n",
      "\n",
      "[5 rows x 35 columns]\n",
      "\n",
      "Missing values per column:\n",
      "Age                         0\n",
      "Attrition                   0\n",
      "BusinessTravel              0\n",
      "DailyRate                   0\n",
      "Department                  0\n",
      "DistanceFromHome            0\n",
      "Education                   0\n",
      "EducationField              0\n",
      "EmployeeCount               0\n",
      "EmployeeNumber              0\n",
      "EnvironmentSatisfaction     0\n",
      "Gender                      0\n",
      "HourlyRate                  0\n",
      "JobInvolvement              0\n",
      "JobLevel                    0\n",
      "JobRole                     0\n",
      "JobSatisfaction             0\n",
      "MaritalStatus               0\n",
      "MonthlyIncome               0\n",
      "MonthlyRate                 0\n",
      "NumCompaniesWorked          0\n",
      "Over18                      0\n",
      "OverTime                    0\n",
      "PercentSalaryHike           0\n",
      "PerformanceRating           0\n",
      "RelationshipSatisfaction    0\n",
      "StandardHours               0\n",
      "StockOptionLevel            0\n",
      "TotalWorkingYears           0\n",
      "TrainingTimesLastYear       0\n",
      "WorkLifeBalance             0\n",
      "YearsAtCompany              0\n",
      "YearsInCurrentRole          0\n",
      "YearsSinceLastPromotion     0\n",
      "YearsWithCurrManager        0\n",
      "dtype: int64\n",
      "\n",
      "Cleaned data info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1470 entries, 0 to 1469\n",
      "Data columns (total 32 columns):\n",
      " #   Column                    Non-Null Count  Dtype   \n",
      "---  ------                    --------------  -----   \n",
      " 0   Age                       1470 non-null   int64   \n",
      " 1   Attrition                 1470 non-null   category\n",
      " 2   BusinessTravel            1470 non-null   category\n",
      " 3   DailyRate                 1470 non-null   int64   \n",
      " 4   Department                1470 non-null   category\n",
      " 5   DistanceFromHome          1470 non-null   int64   \n",
      " 6   Education                 1470 non-null   int64   \n",
      " 7   EducationField            1470 non-null   category\n",
      " 8   EmployeeNumber            1470 non-null   int64   \n",
      " 9   EnvironmentSatisfaction   1470 non-null   int64   \n",
      " 10  Gender                    1470 non-null   category\n",
      " 11  HourlyRate                1470 non-null   int64   \n",
      " 12  JobInvolvement            1470 non-null   int64   \n",
      " 13  JobLevel                  1470 non-null   int64   \n",
      " 14  JobRole                   1470 non-null   category\n",
      " 15  JobSatisfaction           1470 non-null   int64   \n",
      " 16  MaritalStatus             1470 non-null   category\n",
      " 17  MonthlyIncome             1470 non-null   int64   \n",
      " 18  MonthlyRate               1470 non-null   int64   \n",
      " 19  NumCompaniesWorked        1470 non-null   int64   \n",
      " 20  OverTime                  1470 non-null   category\n",
      " 21  PercentSalaryHike         1470 non-null   int64   \n",
      " 22  PerformanceRating         1470 non-null   int64   \n",
      " 23  RelationshipSatisfaction  1470 non-null   int64   \n",
      " 24  StockOptionLevel          1470 non-null   int64   \n",
      " 25  TotalWorkingYears         1470 non-null   int64   \n",
      " 26  TrainingTimesLastYear     1470 non-null   int64   \n",
      " 27  WorkLifeBalance           1470 non-null   int64   \n",
      " 28  YearsAtCompany            1470 non-null   int64   \n",
      " 29  YearsInCurrentRole        1470 non-null   int64   \n",
      " 30  YearsSinceLastPromotion   1470 non-null   int64   \n",
      " 31  YearsWithCurrManager      1470 non-null   int64   \n",
      "dtypes: category(8), int64(24)\n",
      "memory usage: 288.6 KB\n",
      "None\n",
      "Epoch 1/20 - Train Loss: 0.5917 - Val Loss: 0.4760\n",
      "Epoch 2/20 - Train Loss: 0.4252 - Val Loss: 0.4225\n",
      "Epoch 3/20 - Train Loss: 0.3857 - Val Loss: 0.4071\n",
      "Epoch 4/20 - Train Loss: 0.3636 - Val Loss: 0.3946\n",
      "Epoch 5/20 - Train Loss: 0.3448 - Val Loss: 0.3858\n",
      "Epoch 6/20 - Train Loss: 0.3294 - Val Loss: 0.3805\n",
      "Epoch 7/20 - Train Loss: 0.3143 - Val Loss: 0.3742\n",
      "Epoch 8/20 - Train Loss: 0.2985 - Val Loss: 0.3725\n",
      "Epoch 9/20 - Train Loss: 0.2882 - Val Loss: 0.3703\n",
      "Epoch 10/20 - Train Loss: 0.2727 - Val Loss: 0.3757\n",
      "Epoch 11/20 - Train Loss: 0.2628 - Val Loss: 0.3784\n",
      "Epoch 12/20 - Train Loss: 0.2541 - Val Loss: 0.3819\n",
      "Epoch 13/20 - Train Loss: 0.2435 - Val Loss: 0.3829\n",
      "Epoch 14/20 - Train Loss: 0.2321 - Val Loss: 0.4056\n",
      "Epoch 15/20 - Train Loss: 0.2228 - Val Loss: 0.3956\n",
      "Epoch 16/20 - Train Loss: 0.2147 - Val Loss: 0.4007\n",
      "Epoch 17/20 - Train Loss: 0.2061 - Val Loss: 0.4055\n",
      "Epoch 18/20 - Train Loss: 0.1960 - Val Loss: 0.4039\n",
      "Epoch 19/20 - Train Loss: 0.1869 - Val Loss: 0.4279\n",
      "Epoch 20/20 - Train Loss: 0.1783 - Val Loss: 0.4283\n",
      "Test Accuracy: 86.43%\n",
      "Epoch 1/50 - Train Loss: 0.5883 - Val Loss: 0.5768\n",
      "Epoch 2/50 - Train Loss: 0.5520 - Val Loss: 0.5352\n",
      "Epoch 3/50 - Train Loss: 0.5069 - Val Loss: 0.5071\n",
      "Epoch 4/50 - Train Loss: 0.4801 - Val Loss: 0.4895\n",
      "Epoch 5/50 - Train Loss: 0.4714 - Val Loss: 0.4703\n",
      "Epoch 6/50 - Train Loss: 0.4334 - Val Loss: 0.4518\n",
      "Epoch 7/50 - Train Loss: 0.4367 - Val Loss: 0.4494\n",
      "Epoch 8/50 - Train Loss: 0.4220 - Val Loss: 0.4310\n",
      "Epoch 9/50 - Train Loss: 0.3970 - Val Loss: 0.4195\n",
      "Epoch 10/50 - Train Loss: 0.4094 - Val Loss: 0.4179\n",
      "Epoch 11/50 - Train Loss: 0.3968 - Val Loss: 0.4117\n",
      "Epoch 12/50 - Train Loss: 0.3918 - Val Loss: 0.4091\n",
      "Epoch 13/50 - Train Loss: 0.3870 - Val Loss: 0.4107\n",
      "Epoch 14/50 - Train Loss: 0.3921 - Val Loss: 0.4001\n",
      "Epoch 15/50 - Train Loss: 0.3696 - Val Loss: 0.4012\n",
      "Epoch 16/50 - Train Loss: 0.3672 - Val Loss: 0.3958\n",
      "Epoch 17/50 - Train Loss: 0.3704 - Val Loss: 0.3840\n",
      "Epoch 18/50 - Train Loss: 0.3632 - Val Loss: 0.3895\n",
      "Epoch 19/50 - Train Loss: 0.3594 - Val Loss: 0.3904\n",
      "Epoch 20/50 - Train Loss: 0.3600 - Val Loss: 0.3810\n",
      "Epoch 21/50 - Train Loss: 0.3543 - Val Loss: 0.3793\n",
      "Epoch 22/50 - Train Loss: 0.3524 - Val Loss: 0.3838\n",
      "Epoch 23/50 - Train Loss: 0.3399 - Val Loss: 0.3792\n",
      "Epoch 24/50 - Train Loss: 0.3486 - Val Loss: 0.3812\n",
      "Epoch 25/50 - Train Loss: 0.3381 - Val Loss: 0.3812\n",
      "Epoch 26/50 - Train Loss: 0.3489 - Val Loss: 0.3800\n",
      "Epoch 27/50 - Train Loss: 0.3354 - Val Loss: 0.3787\n",
      "Epoch 28/50 - Train Loss: 0.3411 - Val Loss: 0.3801\n",
      "Epoch 29/50 - Train Loss: 0.3237 - Val Loss: 0.3773\n",
      "Epoch 30/50 - Train Loss: 0.3365 - Val Loss: 0.3761\n",
      "Epoch 31/50 - Train Loss: 0.3308 - Val Loss: 0.3764\n",
      "Epoch 32/50 - Train Loss: 0.3191 - Val Loss: 0.3791\n",
      "Epoch 33/50 - Train Loss: 0.3266 - Val Loss: 0.3765\n",
      "Epoch 34/50 - Train Loss: 0.3357 - Val Loss: 0.3722\n",
      "Epoch 35/50 - Train Loss: 0.3149 - Val Loss: 0.3716\n",
      "Epoch 36/50 - Train Loss: 0.3293 - Val Loss: 0.3731\n",
      "Epoch 37/50 - Train Loss: 0.3174 - Val Loss: 0.3759\n",
      "Epoch 38/50 - Train Loss: 0.3314 - Val Loss: 0.3772\n",
      "Epoch 39/50 - Train Loss: 0.3113 - Val Loss: 0.3761\n",
      "Epoch 40/50 - Train Loss: 0.3251 - Val Loss: 0.3801\n",
      "Early stopping!\n",
      "Test Accuracy: 86.43%\n"
     ]
    }
   ],
   "source": [
    "# import\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"HR-Employee-Attrition.csv\")\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "cols_to_drop = ['EmployeeCount', 'Over18', 'StandardHours']\n",
    "df.drop(columns=[col for col in cols_to_drop if col in df.columns], inplace=True)\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        if df[col].dtype in ['int64', 'float64']:\n",
    "            df[col].fillna(df[col].median(), inplace=True)\n",
    "        else:\n",
    "            df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    df[col] = df[col].astype('category')\n",
    "\n",
    "if 'Attrition' in df.columns:\n",
    "    df['Attrition'] = df['Attrition'].apply(lambda x: 1 if x.strip().lower() == 'yes' else 0)\n",
    "\n",
    "print(\"\\nCleaned data info:\")\n",
    "print(df.info())\n",
    "\n",
    "df.to_csv(\"HR-Employee-Attrition_cleaned.csv\", index=False)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = pd.read_csv(\"HR-Employee-Attrition_cleaned.csv\")\n",
    "for col in data.columns:\n",
    "    if data[col].dtype == 'object':\n",
    "        data[col] = data[col].astype('category').cat.codes\n",
    "\n",
    "X = data.drop(\"Attrition\", axis=1).values\n",
    "y = data[\"Attrition\"].values\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# define dataloaders: make sure to have a train, validation and a test loader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# define the model\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "model = FeedForwardNN(input_dim)\n",
    "\n",
    "# define the loss function and the optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# train the model\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * X_batch.size(0)\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    \n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_val_batch, y_val_batch in val_loader:\n",
    "            outputs = model(X_val_batch)\n",
    "            loss = criterion(outputs, y_val_batch)\n",
    "            running_val_loss += loss.item() * X_val_batch.size(0)\n",
    "    val_loss = running_val_loss / len(val_loader.dataset)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {epoch_loss:.4f} - Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# test the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for X_test_batch, y_test_batch in test_loader:\n",
    "        outputs = model(X_test_batch)\n",
    "        predictions = torch.sigmoid(outputs)\n",
    "        predicted = (predictions > 0.5).float()\n",
    "        total += y_test_batch.size(0)\n",
    "        correct += (predicted == y_test_batch).sum().item()\n",
    "accuracy = correct / total\n",
    "print(f\"Test Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "# define the model with Dropout and BatchNorm, try different learning rates and early stopping\n",
    "class FeedForwardNN_DropoutBN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(FeedForwardNN_DropoutBN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "model = FeedForwardNN_DropoutBN(input_dim)\n",
    "\n",
    "learning_rate = 0.0005\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "num_epochs = 50\n",
    "patience = 5\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * X_batch.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_val_batch, y_val_batch in val_loader:\n",
    "            outputs = model(X_val_batch)\n",
    "            loss = criterion(outputs, y_val_batch)\n",
    "            val_loss += loss.item() * X_val_batch.size(0)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        best_model = model.state_dict()\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "\n",
    "model.load_state_dict(best_model)\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for X_test_batch, y_test_batch in test_loader:\n",
    "        outputs = model(X_test_batch)\n",
    "        predictions = torch.sigmoid(outputs)\n",
    "        predicted = (predictions > 0.5).float()\n",
    "        total += y_test_batch.size(0)\n",
    "        correct += (predicted == y_test_batch).sum().item()\n",
    "accuracy = correct / total\n",
    "print(f\"Test Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e90508",
   "metadata": {},
   "source": [
    "## **Step 2 continued: Try Stuff**\n",
    "\n",
    "Use your code above to try different architectures. Make sure to use early stopping! Try adding Dropout and BatchNorm, try different learning rates. How do they affect training and validation performance? \n",
    "\n",
    " **Summarize your observations in a paragraph below:**\n",
    " \n",
    "Using the basic feedforward model without regularization, the training and validation losses gradually decreased over 20 epochs, and the model achieved a test accuracy of around 86%. However, when adding dropout and batch normalization, lowering the learning rate, and using early stopping, the training process became noticeably more stable, with a smoother convergence and reduced overfitting, even though the final test accuracy remained similar. Overall, these enhancements improved training robustness and helped maintain consistent validation performance, highlighting the benefits of regularization and careful hyperparameter tuning in deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8d368c",
   "metadata": {},
   "source": [
    "## **Step 3: Hyperparameter Optimization with Optuna**\n",
    "\n",
    "As you can see, hyperparameter optimization can be tedious. In class we used [optuna](https://optuna.org/#code_examples) to automate the process. Your next task is to wrap your code from Step 2 into an objective which you can then optimize with optuna. Under the [code exaples](https://optuna.org/#code_examples) there is a tab *PyTorch* which should be helpful as it provides a minimal example on how to wrap PyTorch code inside an objective.\n",
    "\n",
    "**Important: Make sure the model is evaluated on a validation set, not the training data!!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "584d1593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /Users/excellentcdx/Applications/anaconda3/lib/python3.12/site-packages (from optuna) (1.13.3)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in /Users/excellentcdx/Applications/anaconda3/lib/python3.12/site-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/excellentcdx/Applications/anaconda3/lib/python3.12/site-packages (from optuna) (24.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /Users/excellentcdx/Applications/anaconda3/lib/python3.12/site-packages (from optuna) (2.0.34)\n",
      "Requirement already satisfied: tqdm in /Users/excellentcdx/Applications/anaconda3/lib/python3.12/site-packages (from optuna) (4.66.5)\n",
      "Requirement already satisfied: PyYAML in /Users/excellentcdx/Applications/anaconda3/lib/python3.12/site-packages (from optuna) (6.0.1)\n",
      "Requirement already satisfied: Mako in /Users/excellentcdx/Applications/anaconda3/lib/python3.12/site-packages (from alembic>=1.5.0->optuna) (1.2.3)\n",
      "Requirement already satisfied: typing-extensions>=4 in /Users/excellentcdx/Applications/anaconda3/lib/python3.12/site-packages (from alembic>=1.5.0->optuna) (4.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /Users/excellentcdx/Applications/anaconda3/lib/python3.12/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
      "Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\n",
      "Downloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: colorlog, optuna\n",
      "Successfully installed colorlog-6.9.0 optuna-4.2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-06 18:40:17,373] A new study created in memory with name: no-name-379529f3-49e3-4435-8143-225e9a00c029\n",
      "/var/folders/bv/f84v89917tl9_4ggnngdtp7r0000gn/T/ipykernel_42014/2513798039.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)\n",
      "/var/folders/bv/f84v89917tl9_4ggnngdtp7r0000gn/T/ipykernel_42014/2513798039.py:9: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform(\"dropout_rate\", 0.1, 0.5)\n",
      "[I 2025-03-06 18:40:17,974] Trial 0 finished with value: 0.85 and parameters: {'learning_rate': 0.0007029968021810993, 'n_units1': 64, 'n_units2': 64, 'dropout_rate': 0.19980482847729727}. Best is trial 0 with value: 0.85.\n",
      "[I 2025-03-06 18:40:18,613] Trial 1 finished with value: 0.85 and parameters: {'learning_rate': 8.51906918304292e-05, 'n_units1': 96, 'n_units2': 16, 'dropout_rate': 0.2621733123565095}. Best is trial 0 with value: 0.85.\n",
      "[I 2025-03-06 18:40:19,267] Trial 2 finished with value: 0.8590909090909091 and parameters: {'learning_rate': 0.0001100115950956175, 'n_units1': 128, 'n_units2': 48, 'dropout_rate': 0.23065064936164179}. Best is trial 2 with value: 0.8590909090909091.\n",
      "[I 2025-03-06 18:40:19,943] Trial 3 finished with value: 0.8409090909090909 and parameters: {'learning_rate': 1.2666212021952563e-05, 'n_units1': 128, 'n_units2': 64, 'dropout_rate': 0.46562573505220906}. Best is trial 2 with value: 0.8590909090909091.\n",
      "[I 2025-03-06 18:40:20,081] Trial 4 finished with value: 0.8545454545454545 and parameters: {'learning_rate': 0.003186920471832685, 'n_units1': 32, 'n_units2': 32, 'dropout_rate': 0.3352730510611704}. Best is trial 2 with value: 0.8590909090909091.\n",
      "[I 2025-03-06 18:40:20,503] Trial 5 finished with value: 0.8590909090909091 and parameters: {'learning_rate': 0.0002682487783088088, 'n_units1': 64, 'n_units2': 48, 'dropout_rate': 0.34147802551889633}. Best is trial 2 with value: 0.8590909090909091.\n",
      "[I 2025-03-06 18:40:21,140] Trial 6 finished with value: 0.8409090909090909 and parameters: {'learning_rate': 1.9583477390750763e-05, 'n_units1': 128, 'n_units2': 32, 'dropout_rate': 0.15489322853903933}. Best is trial 2 with value: 0.8590909090909091.\n",
      "[I 2025-03-06 18:40:21,772] Trial 7 finished with value: 0.8409090909090909 and parameters: {'learning_rate': 8.504429690732352e-05, 'n_units1': 96, 'n_units2': 48, 'dropout_rate': 0.4077676381751476}. Best is trial 2 with value: 0.8590909090909091.\n",
      "[I 2025-03-06 18:40:22,407] Trial 8 finished with value: 0.8409090909090909 and parameters: {'learning_rate': 1.3499394134254102e-05, 'n_units1': 128, 'n_units2': 16, 'dropout_rate': 0.25620143747515417}. Best is trial 2 with value: 0.8590909090909091.\n",
      "[I 2025-03-06 18:40:23,041] Trial 9 finished with value: 0.8409090909090909 and parameters: {'learning_rate': 0.00011369842122218363, 'n_units1': 128, 'n_units2': 16, 'dropout_rate': 0.48803460249051556}. Best is trial 2 with value: 0.8590909090909091.\n",
      "[I 2025-03-06 18:40:23,142] Trial 10 finished with value: 0.8772727272727273 and parameters: {'learning_rate': 0.007455691970096293, 'n_units1': 96, 'n_units2': 48, 'dropout_rate': 0.12025257540349202}. Best is trial 10 with value: 0.8772727272727273.\n",
      "[I 2025-03-06 18:40:23,253] Trial 11 finished with value: 0.8681818181818182 and parameters: {'learning_rate': 0.0036593835497209354, 'n_units1': 96, 'n_units2': 48, 'dropout_rate': 0.10833695657830389}. Best is trial 10 with value: 0.8772727272727273.\n",
      "[I 2025-03-06 18:40:23,338] Trial 12 finished with value: 0.85 and parameters: {'learning_rate': 0.008817586272227904, 'n_units1': 96, 'n_units2': 48, 'dropout_rate': 0.1011334489612388}. Best is trial 10 with value: 0.8772727272727273.\n",
      "[I 2025-03-06 18:40:23,463] Trial 13 finished with value: 0.8545454545454545 and parameters: {'learning_rate': 0.0027426556321383097, 'n_units1': 64, 'n_units2': 64, 'dropout_rate': 0.10458420255520887}. Best is trial 10 with value: 0.8772727272727273.\n",
      "[I 2025-03-06 18:40:23,565] Trial 14 finished with value: 0.8681818181818182 and parameters: {'learning_rate': 0.007013920568805669, 'n_units1': 96, 'n_units2': 32, 'dropout_rate': 0.15747205003334602}. Best is trial 10 with value: 0.8772727272727273.\n",
      "[I 2025-03-06 18:40:23,744] Trial 15 finished with value: 0.8545454545454545 and parameters: {'learning_rate': 0.0014183847582664496, 'n_units1': 32, 'n_units2': 48, 'dropout_rate': 0.16316328950538572}. Best is trial 10 with value: 0.8772727272727273.\n",
      "[I 2025-03-06 18:40:23,885] Trial 16 finished with value: 0.8636363636363636 and parameters: {'learning_rate': 0.0032663527715788244, 'n_units1': 96, 'n_units2': 32, 'dropout_rate': 0.13509265467325163}. Best is trial 10 with value: 0.8772727272727273.\n",
      "[I 2025-03-06 18:40:24,110] Trial 17 finished with value: 0.8636363636363636 and parameters: {'learning_rate': 0.0006311976039482969, 'n_units1': 64, 'n_units2': 64, 'dropout_rate': 0.2097663737584325}. Best is trial 10 with value: 0.8772727272727273.\n",
      "[I 2025-03-06 18:40:24,248] Trial 18 finished with value: 0.8681818181818182 and parameters: {'learning_rate': 0.0014659758749364957, 'n_units1': 96, 'n_units2': 48, 'dropout_rate': 0.2967096217976534}. Best is trial 10 with value: 0.8772727272727273.\n",
      "[I 2025-03-06 18:40:24,368] Trial 19 finished with value: 0.8727272727272727 and parameters: {'learning_rate': 0.00573274255771121, 'n_units1': 64, 'n_units2': 32, 'dropout_rate': 0.18903134983857212}. Best is trial 10 with value: 0.8772727272727273.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:\n",
      "{'learning_rate': 0.007455691970096293, 'n_units1': 96, 'n_units2': 48, 'dropout_rate': 0.12025257540349202}\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna\n",
    "import optuna\n",
    "\n",
    "# Define an objective function to be maximized.\n",
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)\n",
    "    n_units1 = trial.suggest_int(\"n_units1\", 32, 128, step=32)\n",
    "    n_units2 = trial.suggest_int(\"n_units2\", 16, 64, step=16)\n",
    "    dropout_rate = trial.suggest_uniform(\"dropout_rate\", 0.1, 0.5)\n",
    "    \n",
    "    input_dim = X_train_tensor.shape[1]\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(input_dim, n_units1),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(dropout_rate),\n",
    "        nn.Linear(n_units1, n_units2),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(dropout_rate),\n",
    "        nn.Linear(n_units2, 1)\n",
    "    )\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    num_epochs = 50\n",
    "    patience = 5\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * X_batch.size(0)\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        \n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        for X_val_batch, y_val_batch in val_loader:\n",
    "            outputs = model(X_val_batch)\n",
    "            loss = criterion(outputs, y_val_batch)\n",
    "            running_val_loss += loss.item() * X_val_batch.size(0)\n",
    "        val_loss = running_val_loss / len(val_loader.dataset)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                break\n",
    "                \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_val_batch, y_val_batch in val_loader:\n",
    "            outputs = model(X_val_batch)\n",
    "            predictions = torch.sigmoid(outputs)\n",
    "            predicted = (predictions > 0.5).float()\n",
    "            total += y_val_batch.size(0)\n",
    "            correct += (predicted == y_val_batch).sum().item()\n",
    "    val_accuracy = correct / total\n",
    "    return val_accuracy\n",
    "\n",
    "# Create a study object\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "\n",
    "# Optimize the objective function.\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Print out the best parameters.\n",
    "print(\"Best parameters:\")\n",
    "print(study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f0b007",
   "metadata": {},
   "source": [
    "## **Step 3 continued: Insights**\n",
    "\n",
    "Did you find the hyperparameter search helpful? Does it help to increase the number of trials in the optimization? Note that so far we have used the simplest version of optuna which has many nice features. Can you discover more useful features by browsing the optuna website? (Hint: try pruning)\n",
    "\n",
    "I found that using Optuna for hyperparameter optimization was extremely helpful in tuning my model. By increasing the number of trials, I was able to explore a broader range of parameter combinations, which sometimes led to better validation performance. I observed that even small adjustments in the learning rate, the number of hidden units, or the dropout rate had a noticeable impact on model accuracy. Additionally, I discovered that incorporating features like pruning can further enhance the efficiency of the search process by terminating unpromising trials early. Overall, I learned that automated hyperparameter optimization not only simplifies the tuning process but also provides valuable insights into how sensitive my model is to various hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54312a3",
   "metadata": {},
   "source": [
    "## **Step 4: Final Training**\n",
    "\n",
    "Now that you have found a good hyperparameter setting the validation set is no longer needed. The last step is to combine the training and validation set into a combined training set and retrain the model under the best parameter setting found. Report your final loss on your test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c61ca82-9723-47a5-b65e-6ba070011322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (1470, 35)\n",
      "   Age Attrition     BusinessTravel  DailyRate              Department  \\\n",
      "0   41       Yes      Travel_Rarely       1102                   Sales   \n",
      "1   49        No  Travel_Frequently        279  Research & Development   \n",
      "2   37       Yes      Travel_Rarely       1373  Research & Development   \n",
      "3   33        No  Travel_Frequently       1392  Research & Development   \n",
      "4   27        No      Travel_Rarely        591  Research & Development   \n",
      "\n",
      "   DistanceFromHome  Education EducationField  EmployeeCount  EmployeeNumber  \\\n",
      "0                 1          2  Life Sciences              1               1   \n",
      "1                 8          1  Life Sciences              1               2   \n",
      "2                 2          2          Other              1               4   \n",
      "3                 3          4  Life Sciences              1               5   \n",
      "4                 2          1        Medical              1               7   \n",
      "\n",
      "   ...  RelationshipSatisfaction StandardHours  StockOptionLevel  \\\n",
      "0  ...                         1            80                 0   \n",
      "1  ...                         4            80                 1   \n",
      "2  ...                         2            80                 0   \n",
      "3  ...                         3            80                 0   \n",
      "4  ...                         4            80                 1   \n",
      "\n",
      "   TotalWorkingYears  TrainingTimesLastYear WorkLifeBalance  YearsAtCompany  \\\n",
      "0                  8                      0               1               6   \n",
      "1                 10                      3               3              10   \n",
      "2                  7                      3               3               0   \n",
      "3                  8                      3               3               8   \n",
      "4                  6                      3               3               2   \n",
      "\n",
      "  YearsInCurrentRole  YearsSinceLastPromotion  YearsWithCurrManager  \n",
      "0                  4                        0                     5  \n",
      "1                  7                        1                     7  \n",
      "2                  0                        0                     0  \n",
      "3                  7                        3                     0  \n",
      "4                  2                        2                     2  \n",
      "\n",
      "[5 rows x 35 columns]\n",
      "\n",
      "Missing values per column:\n",
      "Age                         0\n",
      "Attrition                   0\n",
      "BusinessTravel              0\n",
      "DailyRate                   0\n",
      "Department                  0\n",
      "DistanceFromHome            0\n",
      "Education                   0\n",
      "EducationField              0\n",
      "EmployeeCount               0\n",
      "EmployeeNumber              0\n",
      "EnvironmentSatisfaction     0\n",
      "Gender                      0\n",
      "HourlyRate                  0\n",
      "JobInvolvement              0\n",
      "JobLevel                    0\n",
      "JobRole                     0\n",
      "JobSatisfaction             0\n",
      "MaritalStatus               0\n",
      "MonthlyIncome               0\n",
      "MonthlyRate                 0\n",
      "NumCompaniesWorked          0\n",
      "Over18                      0\n",
      "OverTime                    0\n",
      "PercentSalaryHike           0\n",
      "PerformanceRating           0\n",
      "RelationshipSatisfaction    0\n",
      "StandardHours               0\n",
      "StockOptionLevel            0\n",
      "TotalWorkingYears           0\n",
      "TrainingTimesLastYear       0\n",
      "WorkLifeBalance             0\n",
      "YearsAtCompany              0\n",
      "YearsInCurrentRole          0\n",
      "YearsSinceLastPromotion     0\n",
      "YearsWithCurrManager        0\n",
      "dtype: int64\n",
      "\n",
      "Cleaned data info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1470 entries, 0 to 1469\n",
      "Data columns (total 32 columns):\n",
      " #   Column                    Non-Null Count  Dtype   \n",
      "---  ------                    --------------  -----   \n",
      " 0   Age                       1470 non-null   int64   \n",
      " 1   Attrition                 1470 non-null   category\n",
      " 2   BusinessTravel            1470 non-null   category\n",
      " 3   DailyRate                 1470 non-null   int64   \n",
      " 4   Department                1470 non-null   category\n",
      " 5   DistanceFromHome          1470 non-null   int64   \n",
      " 6   Education                 1470 non-null   int64   \n",
      " 7   EducationField            1470 non-null   category\n",
      " 8   EmployeeNumber            1470 non-null   int64   \n",
      " 9   EnvironmentSatisfaction   1470 non-null   int64   \n",
      " 10  Gender                    1470 non-null   category\n",
      " 11  HourlyRate                1470 non-null   int64   \n",
      " 12  JobInvolvement            1470 non-null   int64   \n",
      " 13  JobLevel                  1470 non-null   int64   \n",
      " 14  JobRole                   1470 non-null   category\n",
      " 15  JobSatisfaction           1470 non-null   int64   \n",
      " 16  MaritalStatus             1470 non-null   category\n",
      " 17  MonthlyIncome             1470 non-null   int64   \n",
      " 18  MonthlyRate               1470 non-null   int64   \n",
      " 19  NumCompaniesWorked        1470 non-null   int64   \n",
      " 20  OverTime                  1470 non-null   category\n",
      " 21  PercentSalaryHike         1470 non-null   int64   \n",
      " 22  PerformanceRating         1470 non-null   int64   \n",
      " 23  RelationshipSatisfaction  1470 non-null   int64   \n",
      " 24  StockOptionLevel          1470 non-null   int64   \n",
      " 25  TotalWorkingYears         1470 non-null   int64   \n",
      " 26  TrainingTimesLastYear     1470 non-null   int64   \n",
      " 27  WorkLifeBalance           1470 non-null   int64   \n",
      " 28  YearsAtCompany            1470 non-null   int64   \n",
      " 29  YearsInCurrentRole        1470 non-null   int64   \n",
      " 30  YearsSinceLastPromotion   1470 non-null   int64   \n",
      " 31  YearsWithCurrManager      1470 non-null   int64   \n",
      "dtypes: category(8), int64(24)\n",
      "memory usage: 288.6 KB\n",
      "None\n",
      "Epoch 1/50 - Training Loss: 0.4433\n",
      "Epoch 2/50 - Training Loss: 0.3419\n",
      "Epoch 3/50 - Training Loss: 0.3149\n",
      "Epoch 4/50 - Training Loss: 0.2946\n",
      "Epoch 5/50 - Training Loss: 0.2816\n",
      "Epoch 6/50 - Training Loss: 0.2633\n",
      "Epoch 7/50 - Training Loss: 0.2501\n",
      "Epoch 8/50 - Training Loss: 0.2153\n",
      "Epoch 9/50 - Training Loss: 0.1997\n",
      "Epoch 10/50 - Training Loss: 0.1828\n",
      "Epoch 11/50 - Training Loss: 0.1635\n",
      "Epoch 12/50 - Training Loss: 0.1809\n",
      "Epoch 13/50 - Training Loss: 0.1812\n",
      "Epoch 14/50 - Training Loss: 0.1677\n",
      "Epoch 15/50 - Training Loss: 0.1382\n",
      "Epoch 16/50 - Training Loss: 0.1238\n",
      "Epoch 17/50 - Training Loss: 0.1881\n",
      "Epoch 18/50 - Training Loss: 0.1453\n",
      "Epoch 19/50 - Training Loss: 0.1161\n",
      "Epoch 20/50 - Training Loss: 0.1642\n",
      "Epoch 21/50 - Training Loss: 0.1368\n",
      "Epoch 22/50 - Training Loss: 0.1255\n",
      "Epoch 23/50 - Training Loss: 0.1053\n",
      "Epoch 24/50 - Training Loss: 0.0872\n",
      "Epoch 25/50 - Training Loss: 0.0915\n",
      "Epoch 26/50 - Training Loss: 0.0956\n",
      "Epoch 27/50 - Training Loss: 0.0761\n",
      "Epoch 28/50 - Training Loss: 0.0829\n",
      "Epoch 29/50 - Training Loss: 0.0777\n",
      "Epoch 30/50 - Training Loss: 0.0789\n",
      "Epoch 31/50 - Training Loss: 0.0837\n",
      "Epoch 32/50 - Training Loss: 0.0670\n",
      "Epoch 33/50 - Training Loss: 0.0613\n",
      "Epoch 34/50 - Training Loss: 0.0549\n",
      "Epoch 35/50 - Training Loss: 0.0476\n",
      "Epoch 36/50 - Training Loss: 0.0608\n",
      "Epoch 37/50 - Training Loss: 0.0720\n",
      "Epoch 38/50 - Training Loss: 0.0566\n",
      "Epoch 39/50 - Training Loss: 0.0561\n",
      "Epoch 40/50 - Training Loss: 0.0555\n",
      "Epoch 41/50 - Training Loss: 0.0431\n",
      "Epoch 42/50 - Training Loss: 0.0525\n",
      "Epoch 43/50 - Training Loss: 0.0592\n",
      "Epoch 44/50 - Training Loss: 0.0791\n",
      "Epoch 45/50 - Training Loss: 0.0433\n",
      "Epoch 46/50 - Training Loss: 0.0389\n",
      "Epoch 47/50 - Training Loss: 0.0677\n",
      "Epoch 48/50 - Training Loss: 0.0545\n",
      "Epoch 49/50 - Training Loss: 0.0593\n",
      "Epoch 50/50 - Training Loss: 0.0440\n",
      "Final Test Loss: 1.0400\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"HR-Employee-Attrition.csv\")\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "cols_to_drop = ['EmployeeCount', 'Over18', 'StandardHours']\n",
    "df.drop(columns=[col for col in cols_to_drop if col in df.columns], inplace=True)\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        if df[col].dtype in ['int64', 'float64']:\n",
    "            df[col].fillna(df[col].median(), inplace=True)\n",
    "        else:\n",
    "            df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    df[col] = df[col].astype('category')\n",
    "\n",
    "if 'Attrition' in df.columns:\n",
    "    df['Attrition'] = df['Attrition'].apply(lambda x: 1 if x.strip().lower() == 'yes' else 0)\n",
    "\n",
    "print(\"\\nCleaned data info:\")\n",
    "print(df.info())\n",
    "\n",
    "df.to_csv(\"HR-Employee-Attrition_cleaned.csv\", index=False)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 4: Final Training\n",
    "data = pd.read_csv(\"HR-Employee-Attrition_cleaned.csv\")\n",
    "for col in data.columns:\n",
    "    if data[col].dtype == 'object':\n",
    "        data[col] = data[col].astype('category').cat.codes\n",
    "\n",
    "X = data.drop(\"Attrition\", axis=1).values\n",
    "y = data[\"Attrition\"].values\n",
    "\n",
    "# Combine training and validation sets into a single training set (using 85% for training and 15% for testing)\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_trainval = scaler.fit_transform(X_trainval)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_trainval_tensor = torch.tensor(X_trainval, dtype=torch.float32)\n",
    "y_trainval_tensor = torch.tensor(y_trainval, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "trainval_dataset = TensorDataset(X_trainval_tensor, y_trainval_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "trainval_loader = DataLoader(trainval_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 96)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.12025257540349202)\n",
    "        self.fc2 = nn.Linear(96, 48)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.12025257540349202)\n",
    "        self.fc3 = nn.Linear(48, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "input_dim = X_trainval_tensor.shape[1]\n",
    "model = FeedForwardNN(input_dim)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.007455691970096293)\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for X_batch, y_batch in trainval_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * X_batch.size(0)\n",
    "    epoch_loss = running_loss / len(trainval_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        test_loss += loss.item() * X_batch.size(0)\n",
    "test_loss = test_loss / len(test_loader.dataset)\n",
    "print(f\"Final Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bcf39c",
   "metadata": {},
   "source": [
    "## **Final Submission**\n",
    "Upload your submission for Milestone 2 to Canvas. \n",
    "Happy Deep Learning! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Anaconda)",
   "language": "python",
   "name": "anaconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
